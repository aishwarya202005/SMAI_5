{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd\n",
    "eps = np.finfo(float).eps\n",
    "from numpy import log2 as log\n",
    "import math\n",
    "import sys\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Read dataset to pandas dataframe\n",
    "dataset = pd.read_csv('/home/aishwarya/CSIS/SMAI/SMAI_assig/a-5/small_Data.csv')\n",
    "\n",
    "train, validate = np.split(dataset, [int(.8*len(dataset))]) #for sequential data\n",
    "# train, validate = np.split(dataset.sample(frac=1), [int(.8*len(dataset))]) # for random \n",
    "\n",
    "# dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data_X_Y(data1):\n",
    "    X= data1.iloc[:,:-1].values\n",
    "    Y= data1.iloc[:,-1].values\n",
    "    return X,Y\n",
    "    \n",
    "x_train,Y_train=prepare_data_X_Y(train)\n",
    "x_validate,y_validate=prepare_data_X_Y(validate)\n",
    "\n",
    "X=x_train\n",
    "y=Y_train\n",
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(input_neurons,hidden_neurons,output_neurons,n_hidden_layers):\n",
    "    weight_netwrk=[]       \n",
    "    for i in range(n_hidden_layers):\n",
    "        if i!=0:\n",
    "            l=len(weight_netwrk[-1])\n",
    "            input_neurons=l\n",
    "            \n",
    "        hidden_layer = [ { 'weights': np.random.uniform(size=input_neurons)} for i in range(hidden_neurons) ]\n",
    "        weight_netwrk.append(hidden_layer)\n",
    "    \n",
    "    output_layer = [ { 'weights': np.random.uniform(size=hidden_neurons)} for i in range(output_neurons)]\n",
    "    weight_netwrk.append(output_layer)\n",
    "    \n",
    "    return weight_netwrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate_sigmoid(sum):\n",
    "    return (1/(1+np.exp(-sum)))\n",
    "\n",
    "def activate_tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def activate_ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def d_tanh(x):\n",
    "    return 1. - x * x\n",
    "\n",
    "def d_ReLU(x):\n",
    "    return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons=len(X[0])\n",
    "hidden_neurons=input_neurons+1\n",
    "output_neurons=len(np.unique(y))\n",
    "\n",
    "n_hidden_layers=int(raw_input(\"enter number of hidden layers: \"))\n",
    "# act_func=\"sigmoid\"\n",
    "act_func=raw_input(\"enter activation function: \")\n",
    "# loss_func=\"MSE\"\n",
    "loss_func=raw_input(\"enter loss function: \")\n",
    "net=initialize_network(input_neurons,hidden_neurons,output_neurons,n_hidden_layers)\n",
    "\n",
    "# print_network(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation error\n",
    "def negative_log_likelihood(self):\n",
    "    sigmoid_activation = softmax(numpy.dot(self.x, self.W) + self.b)\n",
    "    cross_entropy = - numpy.mean(numpy.sum(self.y * numpy.log(sigmoid_activation) +(1 - self.y) * numpy.log(1 - sigmoid_activation), axis=1))\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(wgt_net,inputs,act_func):\n",
    "    cur_row=inputs\n",
    "    for layer in wgt_net:\n",
    "#     i=0\n",
    "#     while i<len(net):\n",
    "#         print 'layr-'\n",
    "#         print net[i]\n",
    "#         layer=net[i]\n",
    "        prev_input=np.array([])\n",
    "        for neuron in layer:\n",
    "            sum=neuron['weights'].T.dot(cur_row)\n",
    "            \n",
    "            if act_func==\"sigmoid\":\n",
    "                result=activate_sigmoid(sum)\n",
    "            elif act_func==\"relu\":\n",
    "                result=activate_ReLU(sum)\n",
    "            elif act_func==\"tanh\":\n",
    "                result=activate_tanh(sum)\n",
    "                \n",
    "            neuron['result']=result\n",
    "            \n",
    "            prev_input=np.append(prev_input,[result])\n",
    "            \n",
    "        cur_row =prev_input\n",
    "#         i+=1\n",
    "    \n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid(output):\n",
    "    return output*(1.0-output)\n",
    "\n",
    "# expand\n",
    "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
    "    return ce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(wgt_net,row,expected,act_func):\n",
    "    length=len(wgt_net)\n",
    "    for i in reversed(range(length)):\n",
    "        layer=wgt_net[i]\n",
    "        layr_len=len(layer)\n",
    "        errors=np.array([])\n",
    "        if i==length-1:\n",
    "            results=[neuron['result'] for neuron in layer]\n",
    "            res=np.array(results) \n",
    "            errors = expected-res\n",
    "        else:\n",
    "            for j in range(layr_len):\n",
    "                herror=0\n",
    "                nextlayer=wgt_net[i+1]\n",
    "                for neuron in nextlayer:\n",
    "                    herror= herror+(neuron['weights'][j]*neuron['delta'])\n",
    "                    \n",
    "                errors=np.append(errors,[herror])\n",
    "        j=0\n",
    "        while j<layr_len:            \n",
    "            neuron=layer[j]\n",
    "            if act_func==\"sigmoid\":\n",
    "                neuron['delta']=errors[j]*d_sigmoid(neuron['result'])\n",
    "            elif act_func==\"relu\":\n",
    "                neuron['delta']=errors[j]*d_ReLU(neuron['result'])\n",
    "            elif act_func==\"tanh\":\n",
    "                neuron['delta']=errors[j]*d_tanh(neuron['result'])\n",
    "            j+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateWeights(net,input_layr,lrate):\n",
    "    i=0\n",
    "    while i<len(net):\n",
    "#     for i in range(len(net)):\n",
    "        inputs = input_layr\n",
    "        if i!=0:\n",
    "            inputs=[neuron['result'] for neuron in net[i-1]]\n",
    "\n",
    "        for neuron in net[i]:\n",
    "            j=0\n",
    "            while j<len(inputs):\n",
    "                neuron['weights'][j]+=lrate*neuron['delta']*inputs[j]\n",
    "                j+=1\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net,row,act_func):\n",
    "    ans=forward_propagation(net,row,act_func)\n",
    "    return ans\n",
    "\n",
    "def new_weight(net,lrate,row):\n",
    "    updateWeights(net,row,lrate)  \n",
    "\n",
    "\n",
    "def back_prop(net,row,expected,act_func):\n",
    "    back_propagation(net,row,expected,act_func)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(net, epochs,lrate,n_outputs,act_func,loss_func):\n",
    "    errors_list=list()\n",
    "    epoc_list=[]\n",
    "    for epoch in range(epochs):\n",
    "        sum_error=0\n",
    "        for i,row in enumerate(X):\n",
    "            \n",
    "            expected=[0.0 for i in range(n_outputs)]\n",
    "            expected[y[i]]=1\n",
    "            \n",
    "            outputs=evaluate(net,row,act_func)\n",
    "            \n",
    "            if loss_func==\"MSE\":\n",
    "                sum_error+=sum([(expected[j]-outputs[j])**2 for j in range(len(expected))])\n",
    "            else:\n",
    "                sum_error=cross_entropy(outputs,expected)\n",
    "                \n",
    "            back_prop(net,row,expected,act_func)\n",
    "            new_weight(net,lrate,row)\n",
    "#             back_propagation(net,row,expected)\n",
    "#             updateWeights(net,row,lrate)\n",
    "\n",
    "        if epoch%100 ==0:\n",
    "            print('>epoch=%d,error=%.3f'%(epoch,sum_error))\n",
    "            epoc_list.append(epoch)\n",
    "            errors_list.append(sum_error)\n",
    "    return errors_list,epoc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_list,epoc_list=training(net,1000, 0.05,output_neurons,act_func,loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoc_list,errors_list)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel(\"epochs in 10000's\")\n",
    "\n",
    "fname=\"NN_errors_wrt_\"+act_func+\"_\"+loss_func+\"_.png\"\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with a network\n",
    "def predict(network, row,act_func):\n",
    "    outputs = forward_propagation(net, row,act_func)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y=[]\n",
    "for i in range(len(validate)):    \n",
    "    pred=predict(net,x_validate[i],act_func)\n",
    "    output=np.argmax(pred)\n",
    "    print 'pred-',pred\n",
    "    pred_y.append(output)\n",
    "        \n",
    "cor=0\n",
    "for i in range(len(validate)):\n",
    "    if y_validate[i]==pred_y[i]:\n",
    "        cor+=1\n",
    "\n",
    "accuracy=cor*100/len(validate)\n",
    "# print 'pred_y=',pred_y\n",
    "print 'accuracy=', accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

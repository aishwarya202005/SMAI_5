{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "# import dill\n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd\n",
    "eps = np.finfo(float).eps\n",
    "from numpy import log2 as log\n",
    "import math\n",
    "import sys\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Read dataset to pandas dataframe\n",
    "dataset = pd.read_csv('/home/aishwarya/CSIS/SMAI/SMAI_assig/a-5/small_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate = np.split(dataset, [int(.8*len(dataset))]) #for sequential data\n",
    "# train, validate = np.split(dataset.sample(frac=1), [int(.8*len(dataset))]) # for random \n",
    "\n",
    "# dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data_X_Y(data1):\n",
    "    X= data1.iloc[:,:-1].values\n",
    "    Y= data1.iloc[:,-1].values\n",
    "    return X,Y\n",
    "    \n",
    "x_train,Y_train=prepare_data_X_Y(train)\n",
    "x_validate,y_validate=prepare_data_X_Y(validate)\n",
    "\n",
    "\n",
    "len(x_train[0])\n",
    "# X=dataset1\n",
    "# X=(X - dataset.mean().values)/dataset.std().values\n",
    "# type(X)\n",
    "# X\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Lambda functions #\n",
    "\n",
    "# Sigmoid activation and its derivative\n",
    "SIG = lambda x : 1 / (1 + np.exp(-x))\n",
    "dSIG = lambda o : np.multiply(o, (1 - o))\n",
    "\n",
    "# Error function and its derivative\n",
    "ERROR = lambda t, o : np.multiply(0.5, np.multiply((t - o), (t - o)))\n",
    "dERROR = lambda t, o : o - t\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__ (self, inNodes, outNodes, alpha):\n",
    "        # Numer of input and output nodes\n",
    "        self.inNodes = inNodes + 1\n",
    "        self.outNodes = outNodes\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Matrix of weights init + bias weight\n",
    "        self.weights = np.round(np.random.rand(self.inNodes, self.outNodes) - 0.5, 2)\n",
    "    #         print np.round(np.random.rand(inNodes, outNodes) - 0.5, 2)\n",
    "    #         print 'self-weights='\n",
    "    #         print self.weights\n",
    "\n",
    "\n",
    "    def fwd (self, inputs):\n",
    "        # Add the bias value to the input\n",
    "        self.input = np.concatenate((inputs, [1]))#, axis=1)\n",
    "        print 'self input=',self.input\n",
    "\n",
    "        # Sum the inputs and normalize for each output\n",
    "        sums = np.dot(self.input, self.weights)\n",
    "        self.output = SIG(sums)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def bck (self, dL1):\n",
    "        # Derivative of L1 /w respect to the sum\n",
    "        print self.output.shape\n",
    "        dSIG_OUT = dSIG(self.output)\n",
    "        print 'f`z-', dSIG_OUT\n",
    "        print dSIG_OUT.shape\n",
    "        print 'y-y`-', dL1\n",
    "        print dL1.shape\n",
    "        \n",
    "        dL1_SUM = np.multiply(dL1, dSIG_OUT)\n",
    "        print '\\n(y-y^)*f`(z)==',dL1_SUM\n",
    "        print dL1_SUM.shape\n",
    "\n",
    "        # Derivative of L1 /w respect to input (to be passed to L0)\n",
    "        W_T = np.transpose(self.weights)\n",
    "        print '\\n\\nweights=',W_T\n",
    "        dL1_L0 = np.dot(dL1_SUM, W_T)\n",
    "        print '\\n\\ndL1_L0-', dL1_L0\n",
    "        print 'dL1_L0-', dL1_L0.shape\n",
    "#         dL1_L0=np.asarray(dL1_L0)\n",
    "        dL1_L0=dL1_L0.reshape(dL1_L0.shape[0],1)\n",
    "        print dL1_L0.shape\n",
    "        dL1_L0 = np.delete(dL1_L0, -1, 0)\n",
    "        print 'll----------', dL1_L0.shape\n",
    "\n",
    "        # Change the weights using derivative of L1 /w respect to W\n",
    "        input_T = np.transpose(self.input)\n",
    "        \n",
    "        print 'input ki shape-',input_T.shape\n",
    "        input_T=input_T.reshape(input_T.shape[0],1)\n",
    "        dL1_SUM=dL1_SUM.reshape(dL1_SUM.shape[0],1)\n",
    "        dL1_SUM=dL1_SUM.transpose()\n",
    "        \n",
    "        print self.weights.shape\n",
    "        print 'dL1_SUM-',dL1_SUM.shape\n",
    "        dL1_W = np.dot(input_T, dL1_SUM)\n",
    "        print 'self-alpha-', self.alpha\n",
    "        self.weights -= self.alpha * dL1_W\n",
    "        return dL1_L0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__ (self, features, hiddenNeurons, classes, hiddenLayers = 1, alpha = 2):\n",
    "        # Create the first layer\n",
    "        self.layerStack = np.array([Layer(features, hiddenNeurons, alpha)])\n",
    "\n",
    "        # Create the hidden layers\n",
    "        for x in range(hiddenLayers - 1):\n",
    "            self.layerStack = np.append(self.layerStack, [Layer(hiddenNeurons, hiddenNeurons, alpha)])\n",
    "\n",
    "        # Create the output layer\n",
    "        self.layerStack = np.append(self.layerStack, [Layer(hiddenNeurons, classes, alpha)])\n",
    "\n",
    "    def eval(self, inputs):\n",
    "        # Forward the signal through the layers\n",
    "        lastInput = inputs\n",
    "        for l in self.layerStack:\n",
    "            lastInput = l.fwd(lastInput)\n",
    "        ans=np.argmax(lastInput)\n",
    "        print 'outs-',lastInput\n",
    "        print 'outputs-',ans\n",
    "\n",
    "        return lastInput\n",
    "\n",
    "#     def train(self):\n",
    "#         print 'll'\n",
    "        \n",
    "        \n",
    "    def train(self,inputs,target):\n",
    "        print 'hi its train'\n",
    "        for i in range(1000):\n",
    "            j=0\n",
    "            while j+batch_size<=len(inputs):\n",
    "                print 'j=',j\n",
    "                inputVector=inputs[j]\n",
    "                out=self.eval(inputVector)\n",
    "                print 'predicted y-hat-',out\n",
    "\n",
    "                t=target[j]\n",
    "                y=np.zeros(10, dtype = int)\n",
    "                y[t]=1\n",
    "                print 'actual y is-',y\n",
    "                errorVector=ERROR(y,out)\n",
    "                print 'error i=',errorVector\n",
    "                errorDerivative=dERROR(out,y)\n",
    "                print '\\nerror deriv-',errorDerivative\n",
    "#                 print '\\nout-',out\n",
    "#                 output=np.argmax(out)\n",
    "#                 print(output)\n",
    "                print '\\n====going to back prop============'\n",
    "                for l in range(len(self.layerStack)-1, -1 , -1):\n",
    "                    print '==>> layer[',l,']'            \n",
    "                    errorDerivative=self.layerStack[l].bck(errorDerivative)\n",
    "                    print 'errorDerivs recvd',errorDerivative\n",
    "                j+=batch_size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi its train\n",
      "j= 0\n",
      "self input= [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  16  51  57  18   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  50  19   0 165 193 137 165\n",
      " 176 185 166 156 172 174 172 173 172 186 183 193 186 184 181 176 175 174\n",
      " 176 195 236 209 206 151 151 212 152 141 105  89 123 114 108 117 129 132\n",
      " 125 125 120 133 128 123 125 131 132 136 145 144 162 173 224 118  79 192\n",
      " 200 130 198 216 161 172 163 143 136 136 123 130 134 133 141 147 165 158\n",
      " 168 180 202 166 183 201 227  67   0  77  92  19  23 199 105 173 242 204\n",
      " 215 206 202 151 137 110 138 159 198 250 214  46  40 255 229 227 248   1\n",
      "   4  71  71 248 195 254 248 187 204 201 205 214 214 200 221 194 185  29\n",
      "   0 196 230 128  44 241 222 214 241  10   0 150 107  14  98   3 135 113\n",
      "  81 213 196 134 168 211 143  16 100  86  83 255 234 254 255 238 223 205\n",
      " 242   3   0 229 180  39 130  90 101 129 127 246 152 231 229 255  79   0\n",
      "   8 255 184 133 230 223 222 221 216 205 239   0   0 174  76  41 224 213\n",
      " 216 151 199  76  68 239 148 227 189  15 135 116   0   0 172 236 219 232\n",
      " 213 211 229   0   0 148 173   5 213 183 181 176 236  11 116 228 173 185\n",
      " 217 175 226 146   0   1 224 223 222 226 209 214 214   0   0 118 249  70\n",
      " 169 192 164 182 236 230 225 218 213 212 211 227 228 240 229 214 239 245\n",
      " 234 220 208 218 191   0   0 100 139  54 207 228 195 213 217 211 210 202\n",
      " 209 199 228 206 116  64 132 203 100 134 228 238 203 221 164   0   0 105\n",
      " 194  80 170 255 231 226 217 211 207 209 201 231 198   0   0  22   8   1\n",
      "   0   0   0 180 211 225 107   0   0  63 255 109 137 255 220 222 212 213\n",
      " 209 206 204 246 103  20 102  61  30  28  38  39  78  30 137 233  45   0\n",
      "   0   0 245 125 110 255 222 223 210 212 208 207 205 237 109  77 130  36\n",
      "  37  33  26  35  91 244 198 215   3   0   0   0 221 148  99 255 219 226\n",
      " 211 211 209 210 212 224 215 209 244  79  32  25  18  57  62 123 233 167\n",
      "   0   0   9   0 186 173  55 255 224 224 213 206 203 199 196 209 214 221\n",
      " 213  59  80  22  17  45 205 151 210 148   0   0   1   0 175 194 160 235\n",
      " 219 217 204 220 224 220 210 207 193 212 202 125 157 118  73 150 226 231\n",
      " 230 106   0   1   0   0  54 135 119 254 254 245 235 239 248 249 245 237\n",
      " 242 240 244 234 250 230 224 222 186 151 184  56   0   1   0   0   0 117\n",
      " 107 144 142 153 167 161 164 168 173 179 181 180 180 191 184 195 206 200\n",
      " 163 107 161  19   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   1]\n",
      "self input= [  1.00000000e+00   1.00000000e+00   3.14020145e-21   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "   0.00000000e+00   1.61960147e-55   1.00000000e+00]\n",
      "outs- [ 0.46754569  0.54239794  0.35663485  0.42311474  0.62245933  0.37051689\n",
      "  0.33181223  0.41824062  0.73105858  0.47751518]\n",
      "outputs- 8\n",
      "predicted y-hat- [ 0.46754569  0.54239794  0.35663485  0.42311474  0.62245933  0.37051689\n",
      "  0.33181223  0.41824062  0.73105858  0.47751518]\n",
      "actual y is- [0 0 0 0 0 0 0 0 1 0]\n",
      "error i= [ 0.10929949  0.14709776  0.06359421  0.08951304  0.19372781  0.06864138\n",
      "  0.05504968  0.08746261  0.03616474  0.11401037]\n",
      "\n",
      "error deriv- [-0.46754569 -0.54239794 -0.35663485 -0.42311474 -0.62245933 -0.37051689\n",
      " -0.33181223 -0.41824062  0.26894142 -0.47751518]\n",
      "\n",
      "====going to back prop============\n",
      "==>> layer[ 1 ]\n",
      "(10,)\n",
      "f`z- [ 0.24894672  0.24820241  0.22944644  0.24408866  0.23500371  0.23323412\n",
      "  0.22171287  0.2433154   0.19661193  0.24949443]\n",
      "(10,)\n",
      "y-y`- [-0.46754569 -0.54239794 -0.35663485 -0.42311474 -0.62245933 -0.37051689\n",
      " -0.33181223 -0.41824062  0.26894142 -0.47751518]\n",
      "(10,)\n",
      "\n",
      "(y-y^)*f`(z)== [-0.11639397 -0.13462448 -0.0818286  -0.10327751 -0.14628025 -0.08641718\n",
      " -0.07356704 -0.10176439  0.05287709 -0.11913738]\n",
      "(10,)\n",
      "\n",
      "\n",
      "weights= [[ 0.47 -0.32 -0.43  0.05  0.25  0.17 -0.39  0.4   0.02  0.28 -0.29]\n",
      " [ 0.28 -0.21  0.2  -0.4  -0.04 -0.49  0.16  0.27 -0.29  0.29 -0.33]\n",
      " [ 0.04 -0.07  0.38 -0.01 -0.4  -0.02 -0.47 -0.41 -0.1   0.49  0.32]\n",
      " [-0.21 -0.48 -0.2  -0.05  0.01 -0.09  0.35  0.39  0.09  0.33 -0.36]\n",
      " [-0.    0.13 -0.31 -0.04  0.3  -0.07  0.18 -0.11  0.22  0.03  0.3 ]\n",
      " [-0.16 -0.43 -0.4  -0.06 -0.4  -0.42 -0.23  0.39  0.06  0.02 -0.1 ]\n",
      " [-0.47 -0.35 -0.49  0.3  -0.36  0.45 -0.23  0.41  0.39 -0.46 -0.06]\n",
      " [ 0.06  0.44  0.28 -0.11 -0.13 -0.27 -0.28 -0.42 -0.09  0.12 -0.13]\n",
      " [ 0.18 -0.16 -0.3  -0.2  -0.33 -0.33  0.33  0.17  0.1  -0.15  0.48]\n",
      " [ 0.37 -0.31  0.4   0.26 -0.42  0.02 -0.04 -0.33 -0.1   0.25  0.22]]\n",
      "\n",
      "\n",
      "dL1_L0- [-0.06625044  0.14840576  0.03663464  0.01262133  0.07096971  0.07818443\n",
      "  0.08734093 -0.04636358 -0.00409649 -0.16801255  0.0707469 ]\n",
      "dL1_L0- (11,)\n",
      "(11, 1)\n",
      "ll---------- (10, 1)\n",
      "input ki shape- (11,)\n",
      "(11, 10)\n",
      "dL1_SUM- (1, 10)\n",
      "self-alpha- 2\n",
      "errorDerivs recvd [[-0.06625044]\n",
      " [ 0.14840576]\n",
      " [ 0.03663464]\n",
      " [ 0.01262133]\n",
      " [ 0.07096971]\n",
      " [ 0.07818443]\n",
      " [ 0.08734093]\n",
      " [-0.04636358]\n",
      " [-0.00409649]\n",
      " [-0.16801255]]\n",
      "==>> layer[ 0 ]\n",
      "(10,)\n",
      "f`z- [  0.00000000e+00   0.00000000e+00   3.14020145e-21   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.61960147e-55]\n",
      "(10,)\n",
      "y-y`- [[-0.06625044]\n",
      " [ 0.14840576]\n",
      " [ 0.03663464]\n",
      " [ 0.01262133]\n",
      " [ 0.07096971]\n",
      " [ 0.07818443]\n",
      " [ 0.08734093]\n",
      " [-0.04636358]\n",
      " [-0.00409649]\n",
      " [-0.16801255]]\n",
      "(10, 1)\n",
      "\n",
      "(y-y^)*f`(z)== [[ -0.00000000e+00  -0.00000000e+00  -2.08039735e-22  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00  -0.00000000e+00  -0.00000000e+00\n",
      "   -0.00000000e+00  -1.07299314e-56]\n",
      " [  0.00000000e+00   0.00000000e+00   4.66023975e-22   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.40358184e-56]\n",
      " [  0.00000000e+00   0.00000000e+00   1.15040149e-22   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   5.93335166e-57]\n",
      " [  0.00000000e+00   0.00000000e+00   3.96335142e-23   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   2.04415223e-57]\n",
      " [  0.00000000e+00   0.00000000e+00   2.22859189e-22   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   1.14942649e-56]\n",
      " [  0.00000000e+00   0.00000000e+00   2.45514857e-22   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   1.26627616e-56]\n",
      " [  0.00000000e+00   0.00000000e+00   2.74268122e-22   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   1.41457502e-56]\n",
      " [ -0.00000000e+00  -0.00000000e+00  -1.45590972e-22  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00  -0.00000000e+00  -0.00000000e+00\n",
      "   -0.00000000e+00  -7.50905179e-57]\n",
      " [ -0.00000000e+00  -0.00000000e+00  -1.28637978e-23  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00  -0.00000000e+00  -0.00000000e+00\n",
      "   -0.00000000e+00  -6.63467813e-58]\n",
      " [ -0.00000000e+00  -0.00000000e+00  -5.27593238e-22  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00  -0.00000000e+00  -0.00000000e+00\n",
      "   -0.00000000e+00  -2.72113366e-56]]\n",
      "(10, 10)\n",
      "\n",
      "\n",
      "weights= [[-0.07 -0.19  0.32 ..., -0.48 -0.37 -0.34]\n",
      " [ 0.12 -0.32  0.18 ...,  0.21  0.13  0.44]\n",
      " [ 0.49  0.07 -0.06 ..., -0.26 -0.05  0.24]\n",
      " ..., \n",
      " [-0.49  0.2  -0.37 ...,  0.1  -0.18 -0.1 ]\n",
      " [ 0.12 -0.27  0.44 ..., -0.12 -0.48 -0.12]\n",
      " [ 0.41  0.17  0.02 ...,  0.18  0.48  0.19]]\n",
      "\n",
      "\n",
      "dL1_L0- [[ -1.01939470e-22  -1.45627814e-23   1.24823841e-23 ...,   5.40903311e-23\n",
      "    1.04019867e-23  -4.99295364e-23]\n",
      " [  2.28351748e-22   3.26216782e-23  -2.79614385e-23 ...,  -1.21166233e-22\n",
      "   -2.33011987e-23   1.11845754e-22]\n",
      " [  5.63696730e-23   8.05281042e-24  -6.90240893e-24 ...,  -2.99104387e-23\n",
      "   -5.75200744e-24   2.76096357e-23]\n",
      " ..., \n",
      " [ -7.13395763e-23  -1.01913680e-23   8.73545833e-24 ...,   3.78536527e-23\n",
      "    7.27954860e-24  -3.49418333e-23]\n",
      " [ -6.30326092e-24  -9.00465846e-25   7.71827868e-25 ...,   3.34458743e-24\n",
      "    6.43189890e-25  -3.08731147e-24]\n",
      " [ -2.58520687e-22  -3.69315267e-23   3.16555943e-23 ...,   1.37174242e-22\n",
      "    2.63796619e-23  -1.26622377e-22]]\n",
      "dL1_L0- (10, 785)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aishwarya/.local/lib/python2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 7850 into shape (10,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-0c9a441bfcf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# while True:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#       # Get input and return evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-212-f98211c2acb5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, target)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerStack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mprint\u001b[0m \u001b[0;34m'==>> layer['\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m']'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                     \u001b[0merrorDerivative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerStack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrorDerivative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                     \u001b[0;32mprint\u001b[0m \u001b[0;34m'errorDerivs recvd'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrorDerivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mj\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-211-d6e7f5cd5b81>\u001b[0m in \u001b[0;36mbck\u001b[0;34m(self, dL1)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'dL1_L0-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdL1_L0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#         dL1_L0=np.asarray(dL1_L0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mdL1_L0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdL1_L0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdL1_L0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mdL1_L0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdL1_L0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdL1_L0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 7850 into shape (10,1)"
     ]
    }
   ],
   "source": [
    "# XOR Test\n",
    "i = np.matrix([[0, 0],\n",
    "\t       [1, 0],\n",
    "\t       [0, 1],\n",
    "\t       [1, 1]])\n",
    "t = np.matrix([[1],\n",
    " \t       [0],\n",
    "\t       [0],\n",
    " \t       [1]])\n",
    "# Run\n",
    "nn = NeuralNetwork(784, 10, 10)\n",
    "nn.train(x_train, Y_train)\n",
    "# while True:\n",
    "# \t# Get input and return evaluation\n",
    "# \tsample = input(\"Enter an array: \")\n",
    "# \tsample = np.matrix(sample)\n",
    "# \tprint(np.round(nn.eval(sample)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=[0,1,2,3,4,5,6,7,8,9]\n",
    "plt.plot(epochs,errors)\n",
    "plt.xlabel(\"epochs in 10000's\")\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
